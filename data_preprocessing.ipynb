{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8fdc1d4",
   "metadata": {},
   "source": [
    "# Data Preprocessing for PowerCombined and HPC-Kernel-Events Datasets\n",
    "\n",
    "This notebook contains the preprocessing steps for two independent datasets:\n",
    "1. PowerCombined dataset\n",
    "2. HPC-Kernel-Events dataset\n",
    "\n",
    "The preprocessing includes data cleaning, encoding categorical variables, standardization, dimensionality reduction, and handling class imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ced7264",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af4fa40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# For preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# For utilities\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options for pandas\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4791bb15",
   "metadata": {},
   "source": [
    "## Preprocess PowerCombined Dataset\n",
    "\n",
    "In this section, we will:\n",
    "1. Load the PowerCombined dataset\n",
    "2. Explore and clean the data\n",
    "3. Encode categorical features\n",
    "4. Handle class imbalance using SMOTE\n",
    "5. Scale the features using StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6bbeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the PowerCombined dataset\n",
    "# Replace the path with the actual path to your dataset\n",
    "power_combined_path = \"../data/PowerCombined.csv\"  # Update this path\n",
    "\n",
    "try:\n",
    "    power_df = pd.read_csv(power_combined_path)\n",
    "    print(f\"PowerCombined dataset loaded with shape: {power_df.shape}\")\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    print(power_df.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found at {power_combined_path}. Please update the path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2dde42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory data analysis for PowerCombined\n",
    "if 'power_df' in locals():\n",
    "    print(\"Dataset info:\")\n",
    "    print(power_df.info())\n",
    "    \n",
    "    print(\"\\nMissing values:\")\n",
    "    print(power_df.isnull().sum())\n",
    "    \n",
    "    print(\"\\nSummary statistics:\")\n",
    "    print(power_df.describe())\n",
    "    \n",
    "    # Check for categorical columns\n",
    "    categorical_cols = power_df.select_dtypes(include=['object']).columns\n",
    "    print(f\"\\nCategorical columns: {list(categorical_cols)}\")\n",
    "    \n",
    "    # Check class distribution if 'label' or similar column exists\n",
    "    if 'label' in power_df.columns:\n",
    "        print(\"\\nClass distribution:\")\n",
    "        print(power_df['label'].value_counts())\n",
    "    elif 'class' in power_df.columns:\n",
    "        print(\"\\nClass distribution:\")\n",
    "        print(power_df['class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15616a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the PowerCombined dataset\n",
    "if 'power_df' in locals():\n",
    "    # Make a copy of the original data\n",
    "    power_clean = power_df.copy()\n",
    "    \n",
    "    # Drop duplicates if any\n",
    "    original_shape = power_clean.shape\n",
    "    power_clean = power_clean.drop_duplicates()\n",
    "    print(f\"Removed {original_shape[0] - power_clean.shape[0]} duplicate rows\")\n",
    "    \n",
    "    # Handle missing values\n",
    "    for col in power_clean.columns:\n",
    "        if power_clean[col].isnull().sum() > 0:\n",
    "            if power_clean[col].dtype in ['int64', 'float64']:\n",
    "                # Fill numeric columns with median\n",
    "                power_clean[col] = power_clean[col].fillna(power_clean[col].median())\n",
    "            else:\n",
    "                # Fill categorical columns with mode\n",
    "                power_clean[col] = power_clean[col].fillna(power_clean[col].mode()[0])\n",
    "    \n",
    "    print(f\"After cleaning, dataset shape: {power_clean.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b99d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical features in PowerCombined dataset\n",
    "if 'power_clean' in locals():\n",
    "    # Initialize LabelEncoder\n",
    "    le = LabelEncoder()\n",
    "    \n",
    "    # Identify categorical columns\n",
    "    categorical_cols = power_clean.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    # Encode each categorical column\n",
    "    for col in categorical_cols:\n",
    "        # Exclude target variable if it's one of the categorical columns\n",
    "        if col not in ['label', 'class']:\n",
    "            power_clean[col] = le.fit_transform(power_clean[col])\n",
    "            print(f\"Encoded column: {col}\")\n",
    "    \n",
    "    # Encode target variable if it exists and is categorical\n",
    "    if 'label' in power_clean.columns and power_clean['label'].dtype == 'object':\n",
    "        power_clean['label'] = le.fit_transform(power_clean['label'])\n",
    "        print(\"Encoded target column: label\")\n",
    "    elif 'class' in power_clean.columns and power_clean['class'].dtype == 'object':\n",
    "        power_clean['class'] = le.fit_transform(power_clean['class'])\n",
    "        print(\"Encoded target column: class\")\n",
    "        \n",
    "    print(\"\\nAfter encoding, first 5 rows:\")\n",
    "    print(power_clean.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f00fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target for PowerCombined dataset\n",
    "if 'power_clean' in locals():\n",
    "    # Identify target column\n",
    "    target_col = None\n",
    "    if 'label' in power_clean.columns:\n",
    "        target_col = 'label'\n",
    "    elif 'class' in power_clean.columns:\n",
    "        target_col = 'class'\n",
    "    \n",
    "    if target_col:\n",
    "        X_power = power_clean.drop(columns=[target_col])\n",
    "        y_power = power_clean[target_col]\n",
    "        print(f\"Features shape: {X_power.shape}\")\n",
    "        print(f\"Target shape: {y_power.shape}\")\n",
    "        print(f\"Target distribution:\\n{y_power.value_counts()}\")\n",
    "    else:\n",
    "        print(\"No target column ('label' or 'class') found in the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38957e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE to handle class imbalance in PowerCombined dataset\n",
    "if 'X_power' in locals() and 'y_power' in locals():\n",
    "    from collections import Counter\n",
    "    \n",
    "    print(\"Original class distribution:\", Counter(y_power))\n",
    "    \n",
    "    # Apply SMOTE\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_power_resampled, y_power_resampled = smote.fit_resample(X_power, y_power)\n",
    "    \n",
    "    print(\"Resampled class distribution:\", Counter(y_power_resampled))\n",
    "    print(f\"After SMOTE, features shape: {X_power_resampled.shape}\")\n",
    "    print(f\"After SMOTE, target shape: {y_power_resampled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1315de32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features for PowerCombined dataset\n",
    "if 'X_power_resampled' in locals():\n",
    "    # Initialize StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Scale the features\n",
    "    X_power_scaled = scaler.fit_transform(X_power_resampled)\n",
    "    \n",
    "    print(f\"Scaled features shape: {X_power_scaled.shape}\")\n",
    "    print(\"First 5 rows after scaling:\")\n",
    "    print(pd.DataFrame(X_power_scaled, columns=X_power.columns).head())\n",
    "    \n",
    "    # Create a DataFrame with scaled features and target for easier handling\n",
    "    power_processed = pd.DataFrame(X_power_scaled, columns=X_power.columns)\n",
    "    power_processed[target_col] = y_power_resampled\n",
    "    \n",
    "    print(\"\\nProcessed PowerCombined dataset shape:\", power_processed.shape)\n",
    "    print(\"First 5 rows of processed dataset:\")\n",
    "    print(power_processed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc0d130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the processed PowerCombined dataset\n",
    "if 'power_processed' in locals():\n",
    "    output_path = \"../data/PowerCombined_processed.csv\"\n",
    "    power_processed.to_csv(output_path, index=False)\n",
    "    print(f\"Processed PowerCombined dataset saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b741c1",
   "metadata": {},
   "source": [
    "## Preprocess HPC-Kernel-Events Dataset\n",
    "\n",
    "In this section, we will:\n",
    "1. Load the HPC-Kernel-Events dataset\n",
    "2. Clean the data by filtering and renaming columns\n",
    "3. Standardize numeric features using StandardScaler\n",
    "4. Apply PCA for dimensionality reduction\n",
    "5. Encode labels using LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c502d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the HPC-Kernel-Events dataset\n",
    "# Replace the path with the actual path to your dataset\n",
    "hpc_path = \"../data/HPC-Kernel-Events.csv\"  # Update this path\n",
    "\n",
    "try:\n",
    "    hpc_df = pd.read_csv(hpc_path)\n",
    "    print(f\"HPC-Kernel-Events dataset loaded with shape: {hpc_df.shape}\")\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    print(hpc_df.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found at {hpc_path}. Please update the path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540e4de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory data analysis for HPC-Kernel-Events\n",
    "if 'hpc_df' in locals():\n",
    "    print(\"Dataset info:\")\n",
    "    print(hpc_df.info())\n",
    "    \n",
    "    print(\"\\nMissing values:\")\n",
    "    print(hpc_df.isnull().sum())\n",
    "    \n",
    "    print(\"\\nSummary statistics:\")\n",
    "    print(hpc_df.describe())\n",
    "    \n",
    "    # Check for categorical columns\n",
    "    categorical_cols = hpc_df.select_dtypes(include=['object']).columns\n",
    "    print(f\"\\nCategorical columns: {list(categorical_cols)}\")\n",
    "    \n",
    "    # Check class distribution if 'label' or similar column exists\n",
    "    if 'label' in hpc_df.columns:\n",
    "        print(\"\\nClass distribution:\")\n",
    "        print(hpc_df['label'].value_counts())\n",
    "    elif 'class' in hpc_df.columns:\n",
    "        print(\"\\nClass distribution:\")\n",
    "        print(hpc_df['class'].value_counts())\n",
    "    elif 'kernel_name' in hpc_df.columns:\n",
    "        print(\"\\nKernel distribution:\")\n",
    "        print(hpc_df['kernel_name'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed81f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the HPC-Kernel-Events dataset\n",
    "if 'hpc_df' in locals():\n",
    "    # Make a copy of the original data\n",
    "    hpc_clean = hpc_df.copy()\n",
    "    \n",
    "    # Drop duplicates if any\n",
    "    original_shape = hpc_clean.shape\n",
    "    hpc_clean = hpc_clean.drop_duplicates()\n",
    "    print(f\"Removed {original_shape[0] - hpc_clean.shape[0]} duplicate rows\")\n",
    "    \n",
    "    # Handle missing values\n",
    "    for col in hpc_clean.columns:\n",
    "        if hpc_clean[col].isnull().sum() > 0:\n",
    "            if hpc_clean[col].dtype in ['int64', 'float64']:\n",
    "                # Fill numeric columns with median\n",
    "                hpc_clean[col] = hpc_clean[col].fillna(hpc_clean[col].median())\n",
    "            else:\n",
    "                # Fill categorical columns with mode\n",
    "                hpc_clean[col] = hpc_clean[col].fillna(hpc_clean[col].mode()[0])\n",
    "    \n",
    "    # Filter columns if needed\n",
    "    # hpc_clean = hpc_clean.drop(columns=['unwanted_column1', 'unwanted_column2'])\n",
    "    \n",
    "    print(f\"After cleaning, dataset shape: {hpc_clean.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64a2896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare target variable for HPC-Kernel-Events\n",
    "if 'hpc_clean' in locals():\n",
    "    # Identify the target column\n",
    "    target_col = None\n",
    "    if 'kernel_name' in hpc_clean.columns:\n",
    "        target_col = 'kernel_name'\n",
    "    elif 'label' in hpc_clean.columns:\n",
    "        target_col = 'label'\n",
    "    elif 'class' in hpc_clean.columns:\n",
    "        target_col = 'class'\n",
    "    \n",
    "    if target_col:\n",
    "        # Encode the target variable\n",
    "        le = LabelEncoder()\n",
    "        hpc_clean[target_col] = le.fit_transform(hpc_clean[target_col])\n",
    "        print(f\"Encoded {target_col} with {len(le.classes_)} unique values\")\n",
    "        print(f\"Class mapping: {dict(zip(le.classes_, range(len(le.classes_))))}\")\n",
    "        \n",
    "        # Save the encoder classes for later use\n",
    "        target_classes = le.classes_\n",
    "    else:\n",
    "        print(\"No suitable target column found in the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbda5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode other categorical features in HPC-Kernel-Events dataset\n",
    "if 'hpc_clean' in locals() and target_col is not None:\n",
    "    # Identify categorical columns excluding the target\n",
    "    categorical_cols = [col for col in hpc_clean.select_dtypes(include=['object']).columns \n",
    "                        if col != target_col]\n",
    "    \n",
    "    # Encode each categorical column\n",
    "    for col in categorical_cols:\n",
    "        le = LabelEncoder()\n",
    "        hpc_clean[col] = le.fit_transform(hpc_clean[col])\n",
    "        print(f\"Encoded column: {col}\")\n",
    "    \n",
    "    print(\"\\nAfter encoding, first 5 rows:\")\n",
    "    print(hpc_clean.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d51cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target for HPC-Kernel-Events dataset\n",
    "if 'hpc_clean' in locals() and target_col is not None:\n",
    "    X_hpc = hpc_clean.drop(columns=[target_col])\n",
    "    y_hpc = hpc_clean[target_col]\n",
    "    \n",
    "    print(f\"Features shape: {X_hpc.shape}\")\n",
    "    print(f\"Target shape: {y_hpc.shape}\")\n",
    "    print(f\"Target distribution:\\n{y_hpc.value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c819e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features for HPC-Kernel-Events dataset\n",
    "if 'X_hpc' in locals():\n",
    "    # Initialize StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Scale the features\n",
    "    X_hpc_scaled = scaler.fit_transform(X_hpc)\n",
    "    \n",
    "    print(f\"Scaled features shape: {X_hpc_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996e7ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA for dimensionality reduction\n",
    "if 'X_hpc_scaled' in locals():\n",
    "    # Determine number of components to keep 95% of variance\n",
    "    pca = PCA(n_components=0.95)\n",
    "    X_hpc_pca = pca.fit_transform(X_hpc_scaled)\n",
    "    \n",
    "    print(f\"PCA components: {pca.n_components_}\")\n",
    "    print(f\"Explained variance ratio: {np.sum(pca.explained_variance_ratio_):.4f}\")\n",
    "    print(f\"After PCA, features shape: {X_hpc_pca.shape}\")\n",
    "    \n",
    "    # Plot explained variance ratio\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Cumulative Explained Variance')\n",
    "    plt.title('Explained Variance vs. Number of Components')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5bda00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final processed dataframe for HPC-Kernel-Events\n",
    "if 'X_hpc_pca' in locals() and 'y_hpc' in locals():\n",
    "    # Create column names for PCA components\n",
    "    pca_columns = [f'PC{i+1}' for i in range(X_hpc_pca.shape[1])]\n",
    "    \n",
    "    # Create a DataFrame with PCA features\n",
    "    hpc_processed = pd.DataFrame(X_hpc_pca, columns=pca_columns)\n",
    "    \n",
    "    # Add the target column\n",
    "    hpc_processed[target_col] = y_hpc\n",
    "    \n",
    "    print(\"\\nProcessed HPC-Kernel-Events dataset shape:\", hpc_processed.shape)\n",
    "    print(\"First 5 rows of processed dataset:\")\n",
    "    print(hpc_processed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710cf573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the processed HPC-Kernel-Events dataset\n",
    "if 'hpc_processed' in locals():\n",
    "    output_path = \"../data/HPC-Kernel-Events_processed.csv\"\n",
    "    hpc_processed.to_csv(output_path, index=False)\n",
    "    print(f\"Processed HPC-Kernel-Events dataset saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514c8fbd",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we have performed the following preprocessing steps:\n",
    "\n",
    "### PowerCombined Dataset:\n",
    "1. Loaded and explored the dataset\n",
    "2. Cleaned the data by removing duplicates and handling missing values\n",
    "3. Encoded categorical features using LabelEncoder\n",
    "4. Balanced the dataset using SMOTE\n",
    "5. Scaled features using StandardScaler\n",
    "6. Saved the processed dataset\n",
    "\n",
    "### HPC-Kernel-Events Dataset:\n",
    "1. Loaded and explored the dataset\n",
    "2. Cleaned the data by removing duplicates and handling missing values\n",
    "3. Encoded categorical features and the target variable using LabelEncoder\n",
    "4. Standardized numeric features using StandardScaler\n",
    "5. Applied PCA for dimensionality reduction while preserving 95% of variance\n",
    "6. Saved the processed dataset\n",
    "\n",
    "The processed datasets are ready for modeling and analysis."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
